{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9ocnWO_QsdiN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FF4imvoSse1z"
   },
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"/content/Main Data Set.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LiBHJDHLsqaw"
   },
   "outputs": [],
   "source": [
    "x=data.iloc[:,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1BX-AEXZswfw"
   },
   "outputs": [],
   "source": [
    "y=data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "ELohkb8wsyfx",
    "outputId": "4e78e39b-655e-47fb-839a-4d963a52d370"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-21aba145-438e-4504-bc74-436207c5ebe6\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.670138</td>\n",
       "      <td>0.586168</td>\n",
       "      <td>0.789807</td>\n",
       "      <td>0.569529</td>\n",
       "      <td>0.896276</td>\n",
       "      <td>0.606913</td>\n",
       "      <td>0.689202</td>\n",
       "      <td>0.666287</td>\n",
       "      <td>0.419384</td>\n",
       "      <td>0.778119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363010</td>\n",
       "      <td>0.359452</td>\n",
       "      <td>0.308980</td>\n",
       "      <td>0.388411</td>\n",
       "      <td>0.276964</td>\n",
       "      <td>0.323535</td>\n",
       "      <td>0.356386</td>\n",
       "      <td>0.419597</td>\n",
       "      <td>0.340556</td>\n",
       "      <td>0.203566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.622623</td>\n",
       "      <td>0.612828</td>\n",
       "      <td>0.749603</td>\n",
       "      <td>0.561609</td>\n",
       "      <td>0.858494</td>\n",
       "      <td>0.622171</td>\n",
       "      <td>0.760771</td>\n",
       "      <td>0.713855</td>\n",
       "      <td>0.379224</td>\n",
       "      <td>0.741549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336053</td>\n",
       "      <td>0.413597</td>\n",
       "      <td>0.332691</td>\n",
       "      <td>0.401034</td>\n",
       "      <td>0.298709</td>\n",
       "      <td>0.278295</td>\n",
       "      <td>0.431761</td>\n",
       "      <td>0.355515</td>\n",
       "      <td>0.312453</td>\n",
       "      <td>0.228934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.677310</td>\n",
       "      <td>0.540080</td>\n",
       "      <td>0.694918</td>\n",
       "      <td>0.582878</td>\n",
       "      <td>0.782504</td>\n",
       "      <td>0.564470</td>\n",
       "      <td>0.674928</td>\n",
       "      <td>0.691067</td>\n",
       "      <td>0.368845</td>\n",
       "      <td>0.824662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437042</td>\n",
       "      <td>0.372521</td>\n",
       "      <td>0.316855</td>\n",
       "      <td>0.414318</td>\n",
       "      <td>0.342888</td>\n",
       "      <td>0.369548</td>\n",
       "      <td>0.419606</td>\n",
       "      <td>0.375758</td>\n",
       "      <td>0.360095</td>\n",
       "      <td>0.133659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.699003</td>\n",
       "      <td>0.546206</td>\n",
       "      <td>0.688693</td>\n",
       "      <td>0.668110</td>\n",
       "      <td>0.901110</td>\n",
       "      <td>0.588678</td>\n",
       "      <td>0.688351</td>\n",
       "      <td>0.678569</td>\n",
       "      <td>0.347221</td>\n",
       "      <td>0.794536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407118</td>\n",
       "      <td>0.313939</td>\n",
       "      <td>0.337584</td>\n",
       "      <td>0.408947</td>\n",
       "      <td>0.362332</td>\n",
       "      <td>0.367643</td>\n",
       "      <td>0.421452</td>\n",
       "      <td>0.425308</td>\n",
       "      <td>0.318844</td>\n",
       "      <td>0.202350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.577925</td>\n",
       "      <td>0.709901</td>\n",
       "      <td>0.688234</td>\n",
       "      <td>0.511104</td>\n",
       "      <td>0.910279</td>\n",
       "      <td>0.651897</td>\n",
       "      <td>0.752281</td>\n",
       "      <td>0.647049</td>\n",
       "      <td>0.370606</td>\n",
       "      <td>0.736650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377164</td>\n",
       "      <td>0.449549</td>\n",
       "      <td>0.263653</td>\n",
       "      <td>0.465903</td>\n",
       "      <td>0.313177</td>\n",
       "      <td>0.375177</td>\n",
       "      <td>0.420458</td>\n",
       "      <td>0.460751</td>\n",
       "      <td>0.268537</td>\n",
       "      <td>0.196370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>0.741169</td>\n",
       "      <td>0.652175</td>\n",
       "      <td>0.422804</td>\n",
       "      <td>0.508512</td>\n",
       "      <td>0.463736</td>\n",
       "      <td>0.317828</td>\n",
       "      <td>0.478619</td>\n",
       "      <td>0.280951</td>\n",
       "      <td>0.720227</td>\n",
       "      <td>0.545891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282143</td>\n",
       "      <td>0.589083</td>\n",
       "      <td>0.444875</td>\n",
       "      <td>0.259815</td>\n",
       "      <td>0.459973</td>\n",
       "      <td>0.331036</td>\n",
       "      <td>0.360353</td>\n",
       "      <td>0.309963</td>\n",
       "      <td>0.433842</td>\n",
       "      <td>0.536551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>0.770311</td>\n",
       "      <td>0.530584</td>\n",
       "      <td>0.447036</td>\n",
       "      <td>0.385525</td>\n",
       "      <td>0.519355</td>\n",
       "      <td>0.359627</td>\n",
       "      <td>0.522149</td>\n",
       "      <td>0.511833</td>\n",
       "      <td>0.620028</td>\n",
       "      <td>0.663840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290720</td>\n",
       "      <td>0.453840</td>\n",
       "      <td>0.473137</td>\n",
       "      <td>0.393427</td>\n",
       "      <td>0.363960</td>\n",
       "      <td>0.562967</td>\n",
       "      <td>0.493973</td>\n",
       "      <td>0.423838</td>\n",
       "      <td>0.482669</td>\n",
       "      <td>0.413058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>0.752311</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.377212</td>\n",
       "      <td>0.455005</td>\n",
       "      <td>0.535764</td>\n",
       "      <td>0.278007</td>\n",
       "      <td>0.553484</td>\n",
       "      <td>0.396244</td>\n",
       "      <td>0.698143</td>\n",
       "      <td>0.596044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368523</td>\n",
       "      <td>0.565755</td>\n",
       "      <td>0.462806</td>\n",
       "      <td>0.218220</td>\n",
       "      <td>0.278114</td>\n",
       "      <td>0.372940</td>\n",
       "      <td>0.452606</td>\n",
       "      <td>0.347775</td>\n",
       "      <td>0.351614</td>\n",
       "      <td>0.723992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>0.703642</td>\n",
       "      <td>0.826230</td>\n",
       "      <td>0.386405</td>\n",
       "      <td>0.405879</td>\n",
       "      <td>0.617396</td>\n",
       "      <td>0.257852</td>\n",
       "      <td>0.539068</td>\n",
       "      <td>0.331333</td>\n",
       "      <td>0.634600</td>\n",
       "      <td>0.602477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254838</td>\n",
       "      <td>0.397384</td>\n",
       "      <td>0.454409</td>\n",
       "      <td>0.280427</td>\n",
       "      <td>0.329223</td>\n",
       "      <td>0.192922</td>\n",
       "      <td>0.258212</td>\n",
       "      <td>0.320364</td>\n",
       "      <td>0.393518</td>\n",
       "      <td>0.442190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0.700027</td>\n",
       "      <td>0.517356</td>\n",
       "      <td>0.459972</td>\n",
       "      <td>0.439853</td>\n",
       "      <td>0.481139</td>\n",
       "      <td>0.396422</td>\n",
       "      <td>0.536055</td>\n",
       "      <td>0.337050</td>\n",
       "      <td>0.694987</td>\n",
       "      <td>0.569162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290834</td>\n",
       "      <td>0.505861</td>\n",
       "      <td>0.565475</td>\n",
       "      <td>0.344104</td>\n",
       "      <td>0.524191</td>\n",
       "      <td>0.447155</td>\n",
       "      <td>0.403869</td>\n",
       "      <td>0.453155</td>\n",
       "      <td>0.232406</td>\n",
       "      <td>0.504667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 39 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21aba145-438e-4504-bc74-436207c5ebe6')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-21aba145-438e-4504-bc74-436207c5ebe6 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-21aba145-438e-4504-bc74-436207c5ebe6');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.670138  0.586168  0.789807  0.569529  0.896276  0.606913  0.689202   \n",
       "1     0.622623  0.612828  0.749603  0.561609  0.858494  0.622171  0.760771   \n",
       "2     0.677310  0.540080  0.694918  0.582878  0.782504  0.564470  0.674928   \n",
       "3     0.699003  0.546206  0.688693  0.668110  0.901110  0.588678  0.688351   \n",
       "4     0.577925  0.709901  0.688234  0.511104  0.910279  0.651897  0.752281   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2995  0.741169  0.652175  0.422804  0.508512  0.463736  0.317828  0.478619   \n",
       "2996  0.770311  0.530584  0.447036  0.385525  0.519355  0.359627  0.522149   \n",
       "2997  0.752311  0.711111  0.377212  0.455005  0.535764  0.278007  0.553484   \n",
       "2998  0.703642  0.826230  0.386405  0.405879  0.617396  0.257852  0.539068   \n",
       "2999  0.700027  0.517356  0.459972  0.439853  0.481139  0.396422  0.536055   \n",
       "\n",
       "            7         8         9   ...        29        30        31  \\\n",
       "0     0.666287  0.419384  0.778119  ...  0.363010  0.359452  0.308980   \n",
       "1     0.713855  0.379224  0.741549  ...  0.336053  0.413597  0.332691   \n",
       "2     0.691067  0.368845  0.824662  ...  0.437042  0.372521  0.316855   \n",
       "3     0.678569  0.347221  0.794536  ...  0.407118  0.313939  0.337584   \n",
       "4     0.647049  0.370606  0.736650  ...  0.377164  0.449549  0.263653   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2995  0.280951  0.720227  0.545891  ...  0.282143  0.589083  0.444875   \n",
       "2996  0.511833  0.620028  0.663840  ...  0.290720  0.453840  0.473137   \n",
       "2997  0.396244  0.698143  0.596044  ...  0.368523  0.565755  0.462806   \n",
       "2998  0.331333  0.634600  0.602477  ...  0.254838  0.397384  0.454409   \n",
       "2999  0.337050  0.694987  0.569162  ...  0.290834  0.505861  0.565475   \n",
       "\n",
       "            32        33        34        35        36        37        38  \n",
       "0     0.388411  0.276964  0.323535  0.356386  0.419597  0.340556  0.203566  \n",
       "1     0.401034  0.298709  0.278295  0.431761  0.355515  0.312453  0.228934  \n",
       "2     0.414318  0.342888  0.369548  0.419606  0.375758  0.360095  0.133659  \n",
       "3     0.408947  0.362332  0.367643  0.421452  0.425308  0.318844  0.202350  \n",
       "4     0.465903  0.313177  0.375177  0.420458  0.460751  0.268537  0.196370  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2995  0.259815  0.459973  0.331036  0.360353  0.309963  0.433842  0.536551  \n",
       "2996  0.393427  0.363960  0.562967  0.493973  0.423838  0.482669  0.413058  \n",
       "2997  0.218220  0.278114  0.372940  0.452606  0.347775  0.351614  0.723992  \n",
       "2998  0.280427  0.329223  0.192922  0.258212  0.320364  0.393518  0.442190  \n",
       "2999  0.344104  0.524191  0.447155  0.403869  0.453155  0.232406  0.504667  \n",
       "\n",
       "[3000 rows x 39 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(x)\n",
    "x_scaled=scaler.transform(x)\n",
    "x_scaled=pd.DataFrame(x_scaled)\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GFFtgKDUs06M"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "benLk3d-s3tZ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IAlODAVDs6bu"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)\n",
    "x_train = np.array(x_train).reshape(-1, 39, 1)\n",
    "x_test = np.array(x_test).reshape(-1, 39, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tcdy7k_is-cw"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GRU,Dense,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MICmdiU_tOeM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "a-uMvLCctexQ"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMFcdYVPvXhj",
    "outputId": "29d13769-8979-4644-dfbd-801455011876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "75/75 [==============================] - 1s 6ms/step - loss: 0.0337 - accuracy: 0.0483 - val_loss: 0.0228 - val_accuracy: 0.1400\n",
      "Epoch 2/50\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.1663 - val_loss: 0.0185 - val_accuracy: 0.2200\n",
      "Epoch 3/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0161 - accuracy: 0.2237 - val_loss: 0.0160 - val_accuracy: 0.2200\n",
      "Epoch 4/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0146 - accuracy: 0.2425 - val_loss: 0.0139 - val_accuracy: 0.2200\n",
      "Epoch 5/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0140 - accuracy: 0.2488 - val_loss: 0.0124 - val_accuracy: 0.2250\n",
      "Epoch 6/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0133 - accuracy: 0.2587 - val_loss: 0.0114 - val_accuracy: 0.2333\n",
      "Epoch 7/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 0.2521 - val_loss: 0.0108 - val_accuracy: 0.2333\n",
      "Epoch 8/50\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.0126 - accuracy: 0.2596 - val_loss: 0.0102 - val_accuracy: 0.2767\n",
      "Epoch 9/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.2671 - val_loss: 0.0099 - val_accuracy: 0.2800\n",
      "Epoch 10/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.2688 - val_loss: 0.0097 - val_accuracy: 0.2900\n",
      "Epoch 11/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 0.2846 - val_loss: 0.0095 - val_accuracy: 0.2817\n",
      "Epoch 12/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.2742 - val_loss: 0.0094 - val_accuracy: 0.2817\n",
      "Epoch 13/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.2750 - val_loss: 0.0095 - val_accuracy: 0.2800\n",
      "Epoch 14/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.2783 - val_loss: 0.0093 - val_accuracy: 0.2900\n",
      "Epoch 15/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.2750 - val_loss: 0.0090 - val_accuracy: 0.2767\n",
      "Epoch 16/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.2679 - val_loss: 0.0090 - val_accuracy: 0.2767\n",
      "Epoch 17/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.2654 - val_loss: 0.0090 - val_accuracy: 0.2767\n",
      "Epoch 18/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.2721 - val_loss: 0.0089 - val_accuracy: 0.2683\n",
      "Epoch 19/50\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 0.2704 - val_loss: 0.0089 - val_accuracy: 0.2700\n",
      "Epoch 20/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.2763 - val_loss: 0.0088 - val_accuracy: 0.2733\n",
      "Epoch 21/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.2750 - val_loss: 0.0088 - val_accuracy: 0.2783\n",
      "Epoch 22/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 0.2683 - val_loss: 0.0087 - val_accuracy: 0.2683\n",
      "Epoch 23/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 0.2675 - val_loss: 0.0089 - val_accuracy: 0.2683\n",
      "Epoch 24/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.2637 - val_loss: 0.0087 - val_accuracy: 0.2600\n",
      "Epoch 25/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 0.2692 - val_loss: 0.0088 - val_accuracy: 0.2667\n",
      "Epoch 26/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 0.2763 - val_loss: 0.0087 - val_accuracy: 0.2617\n",
      "Epoch 27/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.2738 - val_loss: 0.0087 - val_accuracy: 0.2600\n",
      "Epoch 28/50\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 0.2783 - val_loss: 0.0086 - val_accuracy: 0.2717\n",
      "Epoch 29/50\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 0.2871 - val_loss: 0.0086 - val_accuracy: 0.2717\n",
      "Epoch 30/50\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.2829 - val_loss: 0.0085 - val_accuracy: 0.2633\n",
      "Epoch 31/50\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.0112 - accuracy: 0.2788 - val_loss: 0.0086 - val_accuracy: 0.2683\n",
      "Epoch 32/50\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.2758 - val_loss: 0.0084 - val_accuracy: 0.2750\n",
      "Epoch 33/50\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.2800 - val_loss: 0.0082 - val_accuracy: 0.2850\n",
      "Epoch 34/50\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.0111 - accuracy: 0.2738 - val_loss: 0.0084 - val_accuracy: 0.2667\n",
      "Epoch 35/50\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 0.2846 - val_loss: 0.0083 - val_accuracy: 0.2767\n",
      "Epoch 36/50\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.2750 - val_loss: 0.0083 - val_accuracy: 0.2700\n",
      "Epoch 37/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 0.2812 - val_loss: 0.0082 - val_accuracy: 0.2750\n",
      "Epoch 38/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.2871 - val_loss: 0.0083 - val_accuracy: 0.2717\n",
      "Epoch 39/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 0.2800 - val_loss: 0.0083 - val_accuracy: 0.2700\n",
      "Epoch 40/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 0.2846 - val_loss: 0.0081 - val_accuracy: 0.2667\n",
      "Epoch 41/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 0.2837 - val_loss: 0.0081 - val_accuracy: 0.2667\n",
      "Epoch 42/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 0.2833 - val_loss: 0.0081 - val_accuracy: 0.2700\n",
      "Epoch 43/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 0.2808 - val_loss: 0.0083 - val_accuracy: 0.2650\n",
      "Epoch 44/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0108 - accuracy: 0.2887 - val_loss: 0.0082 - val_accuracy: 0.2700\n",
      "Epoch 45/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.2846 - val_loss: 0.0081 - val_accuracy: 0.2750\n",
      "Epoch 46/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.2725 - val_loss: 0.0081 - val_accuracy: 0.2717\n",
      "Epoch 47/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 0.2812 - val_loss: 0.0080 - val_accuracy: 0.2833\n",
      "Epoch 48/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.2771 - val_loss: 0.0081 - val_accuracy: 0.2717\n",
      "Epoch 49/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 0.2862 - val_loss: 0.0080 - val_accuracy: 0.2800\n",
      "Epoch 50/50\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 0.2979 - val_loss: 0.0079 - val_accuracy: 0.2767\n",
      "94/94 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    " import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_excel(\"/content/Main Data Set.xlsx\")\n",
    "x = data.iloc[:, :-2]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x)\n",
    "x_scaled = scaler.transform(x)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the autoencoder model\n",
    "input_dim = x_train.shape[1]  # Number of features\n",
    "\n",
    "# Encoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(32, activation='relu')(input_layer)\n",
    "encoded = Dropout(0.2)(encoded)\n",
    "encoded = BatchNormalization()(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile and train the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
    "autoencoder.fit(x_train, x_train, epochs=50, batch_size=32, validation_data=(x_test, x_test))\n",
    "\n",
    "# Extract the encoded features\n",
    "encoder = Model(input_layer, encoded)\n",
    "encoded_features = encoder.predict(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3QPvQnr8cqV",
    "outputId": "bb946eca-3261-4001-e2a4-5a4e2930cd01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.6127164e-02,  9.6908525e-02,  1.4626822e-01, ...,\n",
       "        -6.7630812e-05,  1.0721653e-01,  9.1773733e-02],\n",
       "       [-7.6127164e-02,  9.6908525e-02,  1.4626822e-01, ...,\n",
       "        -6.7630812e-05,  1.0721653e-01,  9.1773733e-02],\n",
       "       [-7.6127164e-02,  9.6908525e-02,  1.4626822e-01, ...,\n",
       "        -6.7630812e-05,  1.0721653e-01,  9.1773733e-02],\n",
       "       ...,\n",
       "       [-7.6127164e-02,  9.6908525e-02,  1.4626822e-01, ...,\n",
       "        -6.7630812e-05,  1.0721653e-01,  9.1773733e-02],\n",
       "       [-7.6127164e-02,  9.6908525e-02,  1.4626822e-01, ...,\n",
       "        -6.7630812e-05,  1.0721653e-01,  9.1773733e-02],\n",
       "       [-7.6127164e-02,  9.6908525e-02,  1.4626822e-01, ...,\n",
       "        -6.7630812e-05,  1.0721653e-01,  9.1773733e-02]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vX3rnV1a8zGh",
    "outputId": "7bd88936-7692-4e21-b112-6a1d69b259d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "75/75 [==============================] - 15s 78ms/step - loss: 0.6415 - accuracy: 0.6358 - val_loss: 0.2409 - val_accuracy: 0.5217\n",
      "Epoch 2/20\n",
      "75/75 [==============================] - 5s 69ms/step - loss: 0.3889 - accuracy: 0.6717 - val_loss: 0.2380 - val_accuracy: 0.4783\n",
      "Epoch 3/20\n",
      "75/75 [==============================] - 4s 56ms/step - loss: 0.3001 - accuracy: 0.7100 - val_loss: 0.2653 - val_accuracy: 0.4783\n",
      "Epoch 4/20\n",
      "75/75 [==============================] - 4s 55ms/step - loss: 0.2419 - accuracy: 0.7538 - val_loss: 0.2665 - val_accuracy: 0.4783\n",
      "Epoch 5/20\n",
      "75/75 [==============================] - 5s 70ms/step - loss: 0.2101 - accuracy: 0.7654 - val_loss: 0.2267 - val_accuracy: 0.5867\n",
      "Epoch 6/20\n",
      "75/75 [==============================] - 4s 56ms/step - loss: 0.1895 - accuracy: 0.7825 - val_loss: 0.1329 - val_accuracy: 0.7933\n",
      "Epoch 7/20\n",
      "75/75 [==============================] - 4s 59ms/step - loss: 0.1631 - accuracy: 0.8108 - val_loss: 0.0735 - val_accuracy: 0.9017\n",
      "Epoch 8/20\n",
      "75/75 [==============================] - 5s 67ms/step - loss: 0.1498 - accuracy: 0.8196 - val_loss: 0.0655 - val_accuracy: 0.9350\n",
      "Epoch 9/20\n",
      "75/75 [==============================] - 4s 56ms/step - loss: 0.1249 - accuracy: 0.8525 - val_loss: 0.0597 - val_accuracy: 0.9417\n",
      "Epoch 10/20\n",
      "75/75 [==============================] - 5s 65ms/step - loss: 0.1145 - accuracy: 0.8712 - val_loss: 0.0494 - val_accuracy: 0.9400\n",
      "Epoch 11/20\n",
      "75/75 [==============================] - 5s 60ms/step - loss: 0.1006 - accuracy: 0.8908 - val_loss: 0.0409 - val_accuracy: 0.9483\n",
      "Epoch 12/20\n",
      "75/75 [==============================] - 4s 55ms/step - loss: 0.0870 - accuracy: 0.9092 - val_loss: 0.0389 - val_accuracy: 0.9567\n",
      "Epoch 13/20\n",
      "75/75 [==============================] - 5s 68ms/step - loss: 0.0753 - accuracy: 0.9237 - val_loss: 0.0402 - val_accuracy: 0.9617\n",
      "Epoch 14/20\n",
      "75/75 [==============================] - 4s 55ms/step - loss: 0.0632 - accuracy: 0.9475 - val_loss: 0.0490 - val_accuracy: 0.9517\n",
      "Epoch 15/20\n",
      "75/75 [==============================] - 4s 55ms/step - loss: 0.0554 - accuracy: 0.9492 - val_loss: 0.0348 - val_accuracy: 0.9717\n",
      "Epoch 16/20\n",
      "75/75 [==============================] - 5s 70ms/step - loss: 0.0473 - accuracy: 0.9625 - val_loss: 0.0191 - val_accuracy: 0.9867\n",
      "Epoch 17/20\n",
      "75/75 [==============================] - 4s 55ms/step - loss: 0.0426 - accuracy: 0.9704 - val_loss: 0.0205 - val_accuracy: 0.9850\n",
      "Epoch 18/20\n",
      "75/75 [==============================] - 4s 54ms/step - loss: 0.0366 - accuracy: 0.9750 - val_loss: 0.0158 - val_accuracy: 0.9817\n",
      "Epoch 19/20\n",
      "75/75 [==============================] - 5s 69ms/step - loss: 0.0347 - accuracy: 0.9754 - val_loss: 0.0170 - val_accuracy: 0.9833\n",
      "Epoch 20/20\n",
      "75/75 [==============================] - 4s 54ms/step - loss: 0.0288 - accuracy: 0.9842 - val_loss: 0.0174 - val_accuracy: 0.9833\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.0174 - accuracy: 0.9833\n",
      "Test Loss: 0.01740182191133499\n",
      "Test Accuracy: 0.9833333492279053\n"
     ]
    }
   ],
   "source": [
    "regressorGRU = Sequential()\n",
    "regressorGRU.add(GRU(units=32, return_sequences=True, input_shape=(encoded_features.shape[1], 1), activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "regressorGRU.add(BatchNormalization())\n",
    "regressorGRU.add(GRU(units=32, return_sequences=True, activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "regressorGRU.add(BatchNormalization())\n",
    "regressorGRU.add(GRU(units=32, return_sequences=True, activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "regressorGRU.add(BatchNormalization())\n",
    "regressorGRU.add(GRU(units=32, activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "regressorGRU.add(BatchNormalization())\n",
    "regressorGRU.add(Dense(units=1))\n",
    "regressorGRU.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Split the encoded features into training and testing sets\n",
    "x_train_enc, x_test_enc, _, _ = train_test_split(encoded_features, y, test_size=0.2, random_state=42)\n",
    "x_train_enc = np.array(x_train_enc).reshape(-1, encoded_features.shape[1], 1)\n",
    "x_test_enc = np.array(x_test_enc).reshape(-1, encoded_features.shape[1], 1)\n",
    "\n",
    "# Implement Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = regressorGRU.fit(x_train_enc, y_train, epochs=20, batch_size=32, validation_data=(x_test_enc, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on test data\n",
    "loss, accuracy = regressorGRU.evaluate(x_test_enc, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "3MfLgcj19Tzh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "G9GVVDnb_1X2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcwLB88r__hs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
